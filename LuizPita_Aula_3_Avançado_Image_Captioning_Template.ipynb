{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LuizPita - Aula 3 - Avançado - Image Captioning - Template.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0c9aebcced3e491c9d513c5bc4019747": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5a9ed69df6114bc990eb73d3734cc11c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_57370cabf14347518c9aaa57bb4d1158",
              "IPY_MODEL_7852a44fb40445a998f6a6e34744f7c6"
            ]
          }
        },
        "5a9ed69df6114bc990eb73d3734cc11c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "57370cabf14347518c9aaa57bb4d1158": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2be75e1940814097a62d415a4d7e7fb0",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_09862cbacfe2476ba69346ce5ee6b43e"
          }
        },
        "7852a44fb40445a998f6a6e34744f7c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ba3888e38c894c309f2093129c8c3254",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [00:00&lt;00:00, 12.7kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1a897554871347e6b1ea331dedd4328e"
          }
        },
        "2be75e1940814097a62d415a4d7e7fb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "09862cbacfe2476ba69346ce5ee6b43e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ba3888e38c894c309f2093129c8c3254": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1a897554871347e6b1ea331dedd4328e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "da04a129b6a34b42b2d52b6f2b779d9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_006f8a88ed4943b890e31cb16f047af7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_77fc5ac1d70d47d1b1908ba397fe92a6",
              "IPY_MODEL_f35e60025f0f4334832d721fa6341eab"
            ]
          }
        },
        "006f8a88ed4943b890e31cb16f047af7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "77fc5ac1d70d47d1b1908ba397fe92a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3ac08e8e07df492594fc3c66929ecedd",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f511cfdf391d4e2f8319febe8382eaa3"
          }
        },
        "f35e60025f0f4334832d721fa6341eab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_143e8ac005194c9297985d972aa4c61c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 755kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_49ef0b8cc9bb45a0baebc615f60b186c"
          }
        },
        "3ac08e8e07df492594fc3c66929ecedd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f511cfdf391d4e2f8319febe8382eaa3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "143e8ac005194c9297985d972aa4c61c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "49ef0b8cc9bb45a0baebc615f60b186c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0788abcabe5540b59e7f41e827b3dade": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_632d71b156d948b792125e585375cbfc",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_bf4561800946463e9ff76026e587a16d",
              "IPY_MODEL_8f4ea07ec84c45dc90a6e62af1805704"
            ]
          }
        },
        "632d71b156d948b792125e585375cbfc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "bf4561800946463e9ff76026e587a16d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_cd1b9f19b4414240b12c9e76e6589285",
            "_dom_classes": [],
            "description": "Epoch 0: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e7ccee7cf0694b47a6adf602b636cb8c"
          }
        },
        "8f4ea07ec84c45dc90a6e62af1805704": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c61519c9a01c4aaaa94d3e09153b1179",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2/2 [00:54&lt;00:00, 27.05s/it, loss=15.975, v_num=46, val_loss=tensor(10.0516, device=&#x27;cuda:0&#x27;), valid_loss=tensor(10.0516, device=&#x27;cuda:0&#x27;), bleu=tensor(0.0465)]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_28832e62e03d433f96158260c8185308"
          }
        },
        "cd1b9f19b4414240b12c9e76e6589285": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e7ccee7cf0694b47a6adf602b636cb8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c61519c9a01c4aaaa94d3e09153b1179": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "28832e62e03d433f96158260c8185308": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c42548e61f4c4bd6ad153fd1e43dc3bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_16308db968e14b75a02f4ad5e2ace84f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ac9e30ab5c3a4410ad3d0dbb46238996",
              "IPY_MODEL_f893473e83a84fbaaff3183a64b57531"
            ]
          }
        },
        "16308db968e14b75a02f4ad5e2ace84f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "ac9e30ab5c3a4410ad3d0dbb46238996": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_254a8279a46846fe91e6fb526c1eec83",
            "_dom_classes": [],
            "description": "Validating: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ffa4ebaa6f8643c39dd5f4fe1257ab0b"
          }
        },
        "f893473e83a84fbaaff3183a64b57531": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f6f2905020f94437bdf4e373adc67a78",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/1 [00:53&lt;00:00,  3.10it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c5134cbc84a1449a9275aa0c741f114b"
          }
        },
        "254a8279a46846fe91e6fb526c1eec83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ffa4ebaa6f8643c39dd5f4fe1257ab0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f6f2905020f94437bdf4e373adc67a78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c5134cbc84a1449a9275aa0c741f114b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuizPitaAlmeida/imagecaptioningwithtransformers/blob/main/LuizPita_Aula_3_Avan%C3%A7ado_Image_Captioning_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNBa1xdIzj2R"
      },
      "source": [
        "##Aula 3 - Avançado - Image Captioning - Template"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9J1vIiA0RyU"
      },
      "source": [
        "TRAIN_MODE=False"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MX9xoHKpUTb",
        "outputId": "e81035cc-0535-4089-9c19-138cd9cbb2ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Oct 13 23:58:55 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.23.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtad8425-bOd"
      },
      "source": [
        "Download do dataset MS COCO reduzido para 23k exemplos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6HxYZD7-Rwz",
        "outputId": "92ddcd72-d6a1-4abb-a11a-eb3e95dfdcb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "!gsutil -m cp -n gs://neuralresearcher_data/unicamp/ia376j_2020s2/aula3/*_IMAGES_coco_5_cap_per_img.hdf5 .\n",
        "!gsutil -m cp -n gs://neuralresearcher_data/unicamp/ia376j_2020s2/aula3/*_CAPTIONS_coco_5_cap_per_img.json ."
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://neuralresearcher_data/unicamp/ia376j_2020s2/aula3/TEST_IMAGES_coco_5_cap_per_img.hdf5...\n",
            "/ [0/3 files][    0.0 B/  4.7 GiB]   0% Done                                    \rCopying gs://neuralresearcher_data/unicamp/ia376j_2020s2/aula3/TRAIN_IMAGES_coco_5_cap_per_img.hdf5...\n",
            "Copying gs://neuralresearcher_data/unicamp/ia376j_2020s2/aula3/VAL_IMAGES_coco_5_cap_per_img.hdf5...\n",
            "| [3/3 files][  4.7 GiB/  4.7 GiB] 100% Done  39.1 MiB/s ETA 00:00:00           \n",
            "Operation completed over 3 objects/4.7 GiB.                                      \n",
            "Copying gs://neuralresearcher_data/unicamp/ia376j_2020s2/aula3/TEST_CAPTIONS_coco_5_cap_per_img.json...\n",
            "Copying gs://neuralresearcher_data/unicamp/ia376j_2020s2/aula3/TRAIN_CAPTIONS_coco_5_cap_per_img.json...\n",
            "Copying gs://neuralresearcher_data/unicamp/ia376j_2020s2/aula3/VAL_CAPTIONS_coco_5_cap_per_img.json...\n",
            "/ [3/3 files][  6.8 MiB/  6.8 MiB] 100% Done                                    \n",
            "Operation completed over 3 objects/6.8 MiB.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qXzyMngNRbX"
      },
      "source": [
        "O dataset deve conter 23520, 1052 e 1047 imagens de treino, validação e teste."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlrDhYV_NlRK"
      },
      "source": [
        "Crie um modelo de image captioning completo similar ao do paper Show, Attend and Tell mas  usando a EfficientNet (pré-treinada) como codificador e usando o decodificador do T5, BART ou Pegasus. Para o decodificador, sugerimos utilizar a biblioteca da Hugginface e adaptar o decodificador para ter como entrada o feature map extraído da imagem.\n",
        "\n",
        "Sugerimos ler a página abaixo para se familizarem como a biblioteca implementa a arquitetura encoder-decoder: https://huggingface.co/transformers/model_doc/encoderdecoder.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nuir07ZG7mZt"
      },
      "source": [
        "## Setup do Hardware e do ambiente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ekmptKmqv1l",
        "outputId": "a081c2c4-d5ad-44e8-a304-c1fd475f400d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        }
      },
      "source": [
        "!pip install --quiet transformers\n",
        "!pip install --quiet pytorch-lightning\n",
        "!pip install --quiet efficientnet_pytorch\n",
        "!pip install --quiet sacrebleu"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.1MB 5.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.0MB 35.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 52.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 63.0MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 512kB 5.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 276kB 24.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 829kB 12.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 92kB 11.7MB/s \n",
            "\u001b[?25h  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 71kB 3.3MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ws_VVj5r7vLA"
      },
      "source": [
        "import os\n",
        "import random\n",
        "import json\n",
        "from collections import Counter, OrderedDict\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import h5py as h5\n",
        "\n",
        "import nltk\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "from torchsummary import summary\n",
        "from transformers import EncoderDecoderModel, AutoTokenizer\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "\n",
        "import sacrebleu"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnnm0zXxr0BZ"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "seed = 0\n",
        "random.seed(seed)\n",
        "torch.random.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFjc_9yQ3b2a"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FCToz-53l-u",
        "outputId": "877f7f91-2d92-47e2-f717-9a8449fc7ceb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116,
          "referenced_widgets": [
            "0c9aebcced3e491c9d513c5bc4019747",
            "5a9ed69df6114bc990eb73d3734cc11c",
            "57370cabf14347518c9aaa57bb4d1158",
            "7852a44fb40445a998f6a6e34744f7c6",
            "2be75e1940814097a62d415a4d7e7fb0",
            "09862cbacfe2476ba69346ce5ee6b43e",
            "ba3888e38c894c309f2093129c8c3254",
            "1a897554871347e6b1ea331dedd4328e",
            "da04a129b6a34b42b2d52b6f2b779d9d",
            "006f8a88ed4943b890e31cb16f047af7",
            "77fc5ac1d70d47d1b1908ba397fe92a6",
            "f35e60025f0f4334832d721fa6341eab",
            "3ac08e8e07df492594fc3c66929ecedd",
            "f511cfdf391d4e2f8319febe8382eaa3",
            "143e8ac005194c9297985d972aa4c61c",
            "49ef0b8cc9bb45a0baebc615f60b186c"
          ]
        }
      },
      "source": [
        "MAX_SEQUENCE_LEN = 49\n",
        "EFFICIENTNET_MODEL = 'efficientnet-b0'\n",
        "BERT_MODEL = 'bert-base-uncased'\n",
        "IMAGE_SIZE = EfficientNet.get_image_size(EFFICIENTNET_MODEL)\n",
        "DEFAULT_BATCH_SIZE = 10\n",
        "TOKENIZER = AutoTokenizer.from_pretrained(BERT_MODEL)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0c9aebcced3e491c9d513c5bc4019747",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "da04a129b6a34b42b2d52b6f2b779d9d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xqubSF8eXzB",
        "outputId": "a750bd52-d970-4af0-d6a5-7155f13378b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "if not TRAIN_MODE:\n",
        "    testing = TOKENIZER.encode('i like pizza.')\n",
        "    print(testing)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[101, 1045, 2066, 10733, 1012, 102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kr_D5AEAegNs",
        "outputId": "a364760e-34c4-4092-9ae8-2cc9ede54221",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "if not TRAIN_MODE:\n",
        "    print(TOKENIZER.decode(testing))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[CLS] i like pizza. [SEP]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2YZS0xP9E1w"
      },
      "source": [
        "## Classe de Vocabulário apartir dos dados de treino"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3cAIzN3z_kp",
        "outputId": "965c238e-0c40-4b2b-d750-58ca28818c71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "class Vocabulary(object):\n",
        "    \"\"\"Simple vocabulary wrapper.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.idx = 0\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.word2idx[word] = self.idx\n",
        "            self.idx2word[self.idx] = word\n",
        "            self.idx += 1\n",
        "\n",
        "    def __call__(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            return self.word2idx['<unk>']\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.word2idx)\n",
        "\n",
        "\n",
        "def build_vocab(json_path, min_freq_to_count_in_vocab):\n",
        "    \"\"\"Build a simple vocabulary wrapper.\"\"\"\n",
        "    counter = Counter()\n",
        "    nltk.download('punkt')\n",
        "\n",
        "    with open(json_path, 'r') as j_file:\n",
        "        captions_file = json.load(j_file)\n",
        "        captions = [caption for five_captions in captions_file\n",
        "                    for caption in five_captions]\n",
        "\n",
        "    for i, caption in enumerate(captions):\n",
        "        tokens = nltk.tokenize.word_tokenize(caption.lower())\n",
        "        counter.update(tokens)\n",
        "\n",
        "    # If the word frequency is less than 'threshold', then the word is\n",
        "    # discarded.\n",
        "    words = [word for word, cnt in counter.items()\n",
        "             if cnt >= min_freq_to_count_in_vocab]\n",
        "\n",
        "    # Create a vocab wrapper and add some special tokens.\n",
        "    vocab = Vocabulary()\n",
        "    vocab.add_word('<pad>')\n",
        "    vocab.add_word('<start>')\n",
        "    vocab.add_word('<end>')\n",
        "    vocab.add_word('<unk>')\n",
        "\n",
        "    # Add the words to the vocabulary.efficient_net\n",
        "    for i, word in enumerate(words):\n",
        "        vocab.add_word(word)\n",
        "    return vocab\n",
        "\n",
        "vocab = build_vocab(\n",
        "    'TRAIN_CAPTIONS_coco_5_cap_per_img.json',\n",
        "    min_freq_to_count_in_vocab=4\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYZUrtVz3M_V"
      },
      "source": [
        "#dict(list(vocab.word2idx.items())[:20])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOc4AMXS9NVf"
      },
      "source": [
        "# Leitura das imagens e captions\n",
        "\n",
        "Aqui também é realizado a tokenização e a conversão de tokens para ids"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJoqy3bnrn-Z"
      },
      "source": [
        "class HDF5ImageDataset(Dataset):\n",
        "    \"\"\"Classe para leitura das imagens\"\"\"\n",
        "\n",
        "    def __init__(self, h5_path, h5_key=None, preprocess=False,\n",
        "                 img_size=IMAGE_SIZE):\n",
        "        \"\"\"Variáveis e métodos criados ao instanciar um objeto\"\"\"\n",
        "        # Uso da lib h5py para fazer carregar o arquivo hdf5 em mémoria\n",
        "        self.h5_file = h5.File(h5_path, 'r')\n",
        "        \n",
        "        # Os arquivos h5py são lidos em uma espécie de dicionário. Durante\n",
        "        # sua criação o usuário definio o nome das chaves. O método a seguir\n",
        "        # permite setar a chave que contém os arquivos de imagem. Caso nenhuma\n",
        "        # chave seja passada o valor assumido é a primeira chave. Em nosso caso\n",
        "        # só existe uma única chave de nome \"images\"\n",
        "        self.set_key(h5_key)\n",
        "        \n",
        "        # Em Visão Computacional é comum aplicarmos alguns preprocessamentos a\n",
        "        # imagem que servirá de entrada da rede. Como exemplo podemos citar\n",
        "        # resize, crop, rodações, espelhamento e normalização. O pytorch possui\n",
        "        # no pacote torchvision uma classe transform que facilita a execução\n",
        "        # desses pré-processamentos. Essa classe recebe como entrada imagens no\n",
        "        # pacote PIL. Em nosso caso, definimos nossa transformação como sendo a\n",
        "        # conversão da imagem em um tensor do pytorch.\n",
        "        self.preprocess = preprocess \n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize(img_size),\n",
        "            transforms.CenterCrop(img_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(\n",
        "                mean=[0.485, 0.456, 0.406],\n",
        "                std=[0.229, 0.224, 0.225]\n",
        "            ),\n",
        "        ])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"As classes de dataset do Pytorch utilizam esse método mágico para\n",
        "        facilitar a iteração sobre datasets. Com ela podemos obter um elemento\n",
        "        do dataset realizando uma simples indexação. \n",
        "        \"\"\"\n",
        "        # Leitura da imagem no índice indicado\n",
        "        image = self.h5_file[self.key][index]\n",
        "\n",
        "        # Verificação do shape da imagem. Nesse tutorial assumimos que as imagem\n",
        "        # possui 3 canais e é lida no formato [channels, rows, cols]. Esse\n",
        "        # método checa o shape e a dimensão das imagens e, quando no formato a\n",
        "        # cima modifica para o formato [rows, cols, channels]     \n",
        "        image = self._check_image(image)\n",
        "        \n",
        "        # Ao ser lida pelo pacote h5py a imagem vem em formato numpy, aqui\n",
        "        # convertemos para PIL image de forma que poderemos usar a\n",
        "        # transformação do torchvision.\n",
        "        image = Image.fromarray(image) \n",
        "\n",
        "        # Aplicação da transformação do pyvision. No exemplo que estamos\n",
        "        # trabalhando, esse processo poderia ser feito de forma simplificada,\n",
        "        # convertendo diretamente o numpy array gerado pelo h5py em um tensor\n",
        "        # torch. Optamos por mostrar a nível de aprendizado a classe transform\n",
        "        if self.preprocess:\n",
        "            image = self.transform(image)\n",
        "        return image\n",
        "  \n",
        "    def __len__(self):\n",
        "        \"\"\"Método mágico usado para informa facilmente o número de elementos\n",
        "        presentes no dataset\n",
        "        \"\"\"\n",
        "        return len(self.h5_file[self.key])\n",
        "\n",
        "    def _check_image(self, image):\n",
        "        \"\"\"Verificação e ajuste nas dimensões e shapes da imagem\"\"\"\n",
        "        # Verifica se a imagem possui apenas três dimensões\n",
        "        if image.ndim == 3:\n",
        "            # Verifica se a imagem está no formato [channels, rows, cols]\n",
        "            # e inverte para o formato [rows, cols, channels] \n",
        "            if image.shape[0] == 3:\n",
        "                return np.moveaxis(image, 0, -1)\n",
        "            \n",
        "            # Verifica se a imagem já se encontra no formato\n",
        "            # [rows, cols, channels], se sim passa adiante \n",
        "            if image.shape[-1] == 3:\n",
        "                return image\n",
        "            \n",
        "            # Casos que fogem os padrões acima são tratados como erro\n",
        "            print('Image does not have 3 channels.')\n",
        "            raise ValueError\n",
        "        \n",
        "        # Caso possua menos ou mais dimensões, um erro é apontado\n",
        "        else:\n",
        "            print('Image dimension is not 3.')\n",
        "            raise ValueError\n",
        "            \n",
        "    def set_key(self, h5_key):\n",
        "        \"\"\"Define a chave do h5py que será usada para carregar as imagens\"\"\"\n",
        "        if h5_key: \n",
        "            self.key = h5_key\n",
        "        else:\n",
        "            self.key = list(self.h5_file[0])\n",
        "\n",
        "    def h5_info(self):\n",
        "        \"\"\"Imprime as chaves que compõem o arquivo de carregado pelo h5py\"\"\"\n",
        "        print('Keys:', list(self.h5_file))\n",
        "\n",
        "    def plot(self, index):\n",
        "        image = self.h5_file[self.key][index]      \n",
        "        image = self._check_image(image)\n",
        "        image = Image.fromarray(image)\n",
        "        plt.imshow(image)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBTgAnyoVynw"
      },
      "source": [
        "if not TRAIN_MODE:    \n",
        "    class MSCOCO_Captions__Dataset_Grouped(Dataset):\n",
        "        def __init__(self, h5_path, json_path, h5_key=None,\n",
        "                    preprocess=True):\n",
        "            with open(json_path, 'r') as annotation_file:\n",
        "                self.captions = json.load(annotation_file)\n",
        "                #self.captions = [caption for five_captions in captions_file\n",
        "                #                 for caption in five_captions]\n",
        "            # Utilizamos a classe anteriormente criada para carregar os arquivos de\n",
        "            # imagens\n",
        "            self.images = HDF5ImageDataset(h5_path, h5_key, preprocess)\n",
        "            self.max_length = MAX_SEQUENCE_LEN\n",
        "\n",
        "        def __getitem__(self, index):\n",
        "            text = self.captions[index]\n",
        "            encoded = self._data_encoder(text)\n",
        "\n",
        "            token_ids = torch.tensor(encoded['input_ids']).type(torch.long)\n",
        "            attention_mask = torch.tensor(encoded['attention_mask']).type(torch.long)\n",
        "            token_type_ids = torch.tensor(encoded['token_type_ids']).type(torch.long)\n",
        "            \n",
        "            return self.images[index], token_ids, attention_mask, token_type_ids\n",
        "            \n",
        "            #caption_as_ids = [\n",
        "                #self.captions2ids(nltk.tokenize.word_tokenize(\n",
        "                #    caption.lower()\n",
        "                #)\n",
        "                #) for caption in self.captions[index]\n",
        "            #]\n",
        "            #return self.images[index], torch.LongTensor(caption_as_ids) #int(index/5)\n",
        "        \n",
        "        def __len__(self):\n",
        "            return len(self.captions)\n",
        "\n",
        "        def _data_encoder(self, text):\n",
        "            return TOKENIZER.encode_plus(\n",
        "                text,\n",
        "                max_length = self.max_length,\n",
        "                padding='max_length',\n",
        "                return_attention_mask=True)\n",
        "\n",
        "        def captions2ids(self, seq):\n",
        "            padded_seq = (\n",
        "                [self.vocab(word) for word in seq] + self.maxseqlen*[self.pad_id]\n",
        "            )\n",
        "            return padded_seq[:self.maxseqlen]\n",
        "\n",
        "        def ids2caption(self, ids_seqs):\n",
        "            return [[self.vocab.idx2word[id] for id in ids_seq]\n",
        "                    for ids_seq in ids_seqs]\n",
        "\n",
        "        def plot(self, index):\n",
        "            self.images.plot(index) #int(index/5)\n",
        "            plt.text(-1, -1, '\\n'.join(self.captions[index]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMq8cWR-snjm"
      },
      "source": [
        "class MSCOCO_Captions__Dataset(Dataset):\n",
        "    def __init__(self, h5_path, json_path, vocab, h5_key=None,\n",
        "                 preprocess=True):\n",
        "        with open(json_path, 'r') as annotation_file:\n",
        "            captions_file = json.load(annotation_file)\n",
        "            self.captions = [caption for five_captions in captions_file\n",
        "                             for caption in five_captions]\n",
        "        # Utilizamos a classe anteriormente criada para carregar os arquivos de\n",
        "        # imagens\n",
        "        self.images = HDF5ImageDataset(h5_path, h5_key, preprocess)\n",
        "        self.vocab = vocab\n",
        "        self.max_length = MAX_SEQUENCE_LEN\n",
        "        self.pad_id = vocab('<pad>')\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = self.captions[index]\n",
        "        encoded = self._data_encoder(text)\n",
        "\n",
        "        token_ids = torch.tensor(encoded['input_ids']).type(torch.long)\n",
        "        attention_mask = torch.tensor(encoded['attention_mask']).type(torch.long)\n",
        "        #token_type_ids = torch.tensor(encoded['token_type_ids']).type(torch.long)\n",
        "        \n",
        "        return self.images[index], token_ids, attention_mask #, token_type_ids \n",
        "        \n",
        "        #caption_as_ids = [\n",
        "        #    self.vocab(word) for word in\n",
        "        #    TOKENIZER.encode(self.captions[index].lower()) #nltk.tokenize.word_tokenize\n",
        "        #]\n",
        "        #padded_seq = caption_as_ids + self.maxseqlen*[self.pad_id]\n",
        "        #return self.images[int(index/5)], torch.LongTensor(padded_seq[:self.maxseqlen])\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.captions)\n",
        "\n",
        "    def _data_encoder(self, text):\n",
        "        return TOKENIZER.encode_plus(\n",
        "            text,\n",
        "            max_length = self.max_length,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True)\n",
        "\n",
        "    def ids2caption(self, ids_seq):\n",
        "        return [self.vocab.idx2word[id] for id in ids_seq]\n",
        "\n",
        "    def plot(self, index):\n",
        "        self.images.plot(int(index/5))\n",
        "        plt.title(self.captions[index])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TactTNKytvRY"
      },
      "source": [
        "if not TRAIN_MODE:\n",
        "    # Validando a classe\n",
        "    test = MSCOCO_Captions__Dataset(\n",
        "        'TEST_IMAGES_coco_5_cap_per_img.hdf5',\n",
        "        'TEST_CAPTIONS_coco_5_cap_per_img.json',\n",
        "        vocab = vocab,\n",
        "        h5_key='images',\n",
        "        preprocess=True\n",
        "    )\n",
        "\n",
        "    # Printando e plotando o primeito elemento\n",
        "    element_id = 0\n",
        "    print(test.captions[0])\n",
        "    image, caption_ids, attention_mask, token_type_ids = test[element_id]\n",
        "    print('Image Tensor (tipo, shape, data):', type(image), image.shape)\n",
        "    print(image)\n",
        "    print('\\nCaption Ids: (tipo, shape, data):', type(caption_ids), caption_ids.shape)\n",
        "    print(caption_ids)\n",
        "    #print(test.ids2caption(caption_ids.tolist())\n",
        "    #for tokens in test.ids2caption(caption_ids.tolist()):\n",
        "    #    print(tokens)\n",
        "    print([TOKENIZER.ids_to_tokens[tokens_ids.item()] for tokens_ids in caption_ids])\n",
        "\n",
        "    print('\\n####### Plot ########')\n",
        "    test.plot(element_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mbdojb_C9fMj"
      },
      "source": [
        "### Criando os dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4jQl3XIufcv",
        "outputId": "876cbd25-32c2-4620-b01e-6a7d2daafd03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "train_dataset = MSCOCO_Captions__Dataset(\n",
        "    'TRAIN_IMAGES_coco_5_cap_per_img.hdf5',\n",
        "    'TRAIN_CAPTIONS_coco_5_cap_per_img.json',\n",
        "    vocab=vocab,\n",
        "    h5_key='images'\n",
        ")\n",
        "\n",
        "valid_dataset = MSCOCO_Captions__Dataset(\n",
        "    'VAL_IMAGES_coco_5_cap_per_img.hdf5',\n",
        "    'VAL_CAPTIONS_coco_5_cap_per_img.json',\n",
        "    vocab=vocab,\n",
        "    h5_key='images'\n",
        ")\n",
        "\n",
        "test_dataset = MSCOCO_Captions__Dataset(\n",
        "    'TEST_IMAGES_coco_5_cap_per_img.hdf5',\n",
        "    'TEST_CAPTIONS_coco_5_cap_per_img.json',\n",
        "    vocab=vocab,\n",
        "    h5_key='images'\n",
        ")\n",
        "\n",
        "print('Length of train_dataset:', len(train_dataset))\n",
        "print('Length of valid_dataset:', len(valid_dataset))\n",
        "print('Length of test_dataset:', len(test_dataset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of train_dataset: 117600\n",
            "Length of valid_dataset: 5260\n",
            "Length of test_dataset: 5235\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Mi3sSXa8FJt"
      },
      "source": [
        "if not TRAIN_MODE:\n",
        "    with open('TRAIN_CAPTIONS_coco_5_cap_per_img.json', 'r') as annotation_file:\n",
        "        ann = json.load(annotation_file)\n",
        "    lengths = [len(caption) for five_captions in ann for caption in five_captions]\n",
        "    print('Max sequence length:', max(lengths))\n",
        "\n",
        "    from matplotlib import pyplot as plt\n",
        "    plt.hist(lengths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-xwlL9p9jCf"
      },
      "source": [
        "# Dataloaders\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZF3-xusxeZ_"
      },
      "source": [
        "if not TRAIN_MODE:\n",
        "    BATCH_SIZE = 5\n",
        "\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=1,\n",
        "        drop_last=True,\n",
        "    )\n",
        "\n",
        "    batch = next(iter(train_dataloader))\n",
        "\n",
        "    print('Tamanho do batch para as imagens:', batch[0].shape)\n",
        "    print('Tamanho do batch para os captions:', batch[1].shape)\n",
        "    print()\n",
        "    print(batch[1][1])\n",
        "    print('Convertendo para caption ...')\n",
        "    #print(test_dataset.ids2caption(batch[1][1].tolist()))\n",
        "    #for tokens in test_dataset.ids2caption(batch[1][1].tolist()):\n",
        "    #    print(tokens)\n",
        "    print([TOKENIZER.ids_to_tokens[tokens_ids.item()]\n",
        "        for tokens_ids in batch[1][1]])\n",
        "    print(batch[2][1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_SLxLT8yORm"
      },
      "source": [
        "if not TRAIN_MODE:\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=DEFAULT_BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=1,\n",
        "        drop_last=True,\n",
        "    )\n",
        "\n",
        "    valid_dataloader = DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size=DEFAULT_BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=1,\n",
        "        drop_last=True,\n",
        "    ) \n",
        "\n",
        "    test_dataloader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=DEFAULT_BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=1,\n",
        "        drop_last=True,\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7s5uE7wRXKy"
      },
      "source": [
        "## lightning datamodule"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2u8RCMQr7sPF"
      },
      "source": [
        "class DataLoaderModule(pl.LightningDataModule):\n",
        "    def __init__(self, train_database, valid_database, test_database):\n",
        "        super().__init__()\n",
        "        self.train_data = train_database\n",
        "        self.valid_data = train_database\n",
        "        self.test_data = train_database\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.train_data,\n",
        "            batch_size=DEFAULT_BATCH_SIZE,\n",
        "            shuffle=False,\n",
        "            num_workers=1,\n",
        "            drop_last=False,\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.valid_data,\n",
        "            batch_size=DEFAULT_BATCH_SIZE,\n",
        "            shuffle=False,\n",
        "            num_workers=1,\n",
        "            drop_last=False,\n",
        "        )\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.test_data,\n",
        "            batch_size=DEFAULT_BATCH_SIZE,\n",
        "            shuffle=False,\n",
        "            num_workers=1,\n",
        "            drop_last=False,\n",
        "        )\n",
        "\n",
        "DATALOADER_MD = DataLoaderModule(\n",
        "    train_database=train_dataset,\n",
        "    valid_database=valid_dataset,\n",
        "    test_database=test_dataset\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JIbSqEepzN9"
      },
      "source": [
        "#Image Encoder\n",
        "\n",
        "Encoder using Efficient Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsyJ7kad1Dvc"
      },
      "source": [
        "# MAX_SEQUENCE_LEN = 49\n",
        "class ImageEncoder(nn.Module):\n",
        "    def __init__(self): #finetune=False\n",
        "        super().__init__()\n",
        "        self.embedding_dim = 768 # Conflito com os embbedings posicionais \n",
        "        self.cnn = EfficientNet.from_pretrained(EFFICIENTNET_MODEL)\n",
        "        \n",
        "        # Freeze model weights to not train\n",
        "        for param in self.cnn.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.encoding_layers = nn.Sequential(OrderedDict([ \n",
        "           ('EmbeddingDim_Conv2d', nn.Conv2d(1280, self.embedding_dim, 3, 1, 1)),\n",
        "           ('Final_DropOut', nn.Dropout(p=0.1))\n",
        "        ]))\n",
        "\n",
        "        # If fine-tuning, only fine-tunes convolutional blocks 2 through 4\n",
        "        #if finetune:\n",
        "        #    for convolutional_layer in list(self.cnn_model.children())[5:]:\n",
        "        #        for param in convolutional_layer.parameters():\n",
        "        #            param.requires_grad = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cnn.extract_features(x)\n",
        "        x = self.encoding_layers(x)\n",
        "        x = x.reshape(-1, self.embedding_dim, MAX_SEQUENCE_LEN)\n",
        "        x = x.transpose(1, 2)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyL5Qp1HTJcv"
      },
      "source": [
        "if not TRAIN_MODE:\n",
        "    test_model = ImageEncoder()\n",
        "    test_model.to(device)\n",
        "    summary(test_model, input_size=(3, IMAGE_SIZE, IMAGE_SIZE))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLi03E_tQxSB"
      },
      "source": [
        "# Transformer Encoder Decoder Model\n",
        "\n",
        "Vamos iniciar nossos testes utilizando o BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWqk0IhfQ6eo"
      },
      "source": [
        "if not TRAIN_MODE:\n",
        "    bert_model = EncoderDecoderModel.from_encoder_decoder_pretrained(\n",
        "        BERT_MODEL, BERT_MODEL\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X315E5hxRlXW"
      },
      "source": [
        "if not TRAIN_MODE:\n",
        "    batch = next(iter(train_dataloader))\n",
        "    input_test = batch[0].to(device)\n",
        "    decoder_input_test = batch[1].to(device)\n",
        "    decoder_input_mask = batch[2].to(device)\n",
        "    encoder_embedding = test_model(input_test)\n",
        "    test_model.to('cpu')\n",
        "    del test_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gk49ICAXSkTO"
      },
      "source": [
        "#print(decoder_input_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmD8CSKDST0x"
      },
      "source": [
        "if not TRAIN_MODE:\n",
        "    bert_model.to(device)\n",
        "    outputs = bert_model(\n",
        "        inputs_embeds=encoder_embedding, decoder_input_ids=decoder_input_test, \n",
        "        decoder_attention_mask=decoder_input_mask, return_dict=True\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbIPRZEyZcCx"
      },
      "source": [
        "if not TRAIN_MODE:\n",
        "    out = outputs['logits'].permute(0,2,1) \n",
        "    #decoder_input_test[0].shape, out[0].shape\n",
        "    #nn.CrossEntropyLoss()(out, decoder_input_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8wDpE52SbLd"
      },
      "source": [
        "#torch.argmax(outputs['logits'][2], dim=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLB924m-wkqi"
      },
      "source": [
        "#test_tensor = torch.Tensor([1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012,\n",
        "#        1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012,\n",
        "#        1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012,\n",
        " #       1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012,\n",
        " #       1012])\n",
        "#TOKENIZER.decode(test_tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMP7w_dUXvJc"
      },
      "source": [
        "if not TRAIN_MODE:\n",
        "    bert_model.to('cpu')\n",
        "    del bert_model\n",
        "    torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5hw1HS2ObYC"
      },
      "source": [
        "# testing sacrebleu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyOd68_l9Rbt"
      },
      "source": [
        "if not TRAIN_MODE:\n",
        "    refs = [\n",
        "            ['The dog bit the man.', 'It was not unexpected.', 'The man bit him first.'],\n",
        "            ['The dog had bit the man.', 'No one was surprised.', 'The man had bitten the dog.']\n",
        "    ]\n",
        "    sys = ['The dog bit the man.', \"It wasn't surprising.\", 'The man had just bitten him.']\n",
        "    bleu = sacrebleu.corpus_bleu(sys, refs)\n",
        "    print(bleu.score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hH_08oRy9m6J"
      },
      "source": [
        "#y_target = [2000, 2002, 1995]\n",
        "#TOKENIZER.decode(y_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fHYrAw9A8nK"
      },
      "source": [
        "if not TRAIN_MODE:\n",
        "    out_test = [\n",
        "                {\"pred_seq\": 'i like dalmatas.', \"target_seq\": 'i hate dalmatas'},\n",
        "                {\"pred_seq\": 'i like chow chow.', \"target_seq\": 'i hate chow chow'},\n",
        "                {\"pred_seq\": 'i like shitsu.', \"target_seq\": 'i hate shitsu'},\n",
        "                {\"pred_seq\": 'i like pastor.', \"target_seq\": 'i hate pastor'},\n",
        "                {\"pred_seq\": 'i like poodle.', \"target_seq\": 'i hate poodle'},\n",
        "\n",
        "                {\"pred_seq\": 'i like pizza.', \"target_seq\": 'i hate pizza'},\n",
        "                {\"pred_seq\": 'i like hotdog.', \"target_seq\": 'i hate hotdog'},\n",
        "                {\"pred_seq\": 'i like burguer.', \"target_seq\": 'i hate burguer'},\n",
        "                {\"pred_seq\": 'i like apple.', \"target_seq\": 'i hate apple'},\n",
        "                {\"pred_seq\": 'i like banana.', \"target_seq\": 'i hate banana'},\n",
        "\n",
        "                {\"pred_seq\": 'i like soccer.', \"target_seq\": 'i hate soccer'},\n",
        "                {\"pred_seq\": 'i like volley.', \"target_seq\": 'i hate volley'},\n",
        "                {\"pred_seq\": 'i like tennis.', \"target_seq\": 'i hate tennis'},\n",
        "                {\"pred_seq\": 'i like video games.', \"target_seq\": 'i video games'},\n",
        "                {\"pred_seq\": 'i like movies.', \"target_seq\": 'i hate movies'}\n",
        "    ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39_z-saZD4At"
      },
      "source": [
        "if not TRAIN_MODE:\n",
        "    predictions = [x[\"pred_seq\"] for x in out_test]\n",
        "    all_targets = [x[\"target_seq\"] for x in out_test]\n",
        "    print(predictions)\n",
        "    print(all_targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUHs-6NyFf76"
      },
      "source": [
        "#grouped_targets = [all_targets[start::5] for start in range(5)]\n",
        "#grouped_targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca1_sq66Gz8z"
      },
      "source": [
        "#grouped_predictions = [predictions[start::5] for start in range(5)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tErDT4uvSZA"
      },
      "source": [
        "#len(predictions[::5]), len(grouped_targets), len(grouped_targets[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVYd9kqIFffe"
      },
      "source": [
        "#bleu = sacrebleu.corpus_bleu(predictions[::5], grouped_targets)\n",
        "#bleu.score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9vWVlnBzD80"
      },
      "source": [
        "#modelo completo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nq0yc67RvhhI"
      },
      "source": [
        "class ImageCaptioning(pl.LightningModule):\n",
        "    \"\"\"Encoder-Decoder Image Caption Model\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = ImageEncoder()\n",
        "        self.decoder = EncoderDecoderModel.from_encoder_decoder_pretrained(\n",
        "            BERT_MODEL, BERT_MODEL\n",
        "        )\n",
        "        self.loss = torch.nn.CrossEntropyLoss()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        images, captions_ids, captions_len_mask  = x\n",
        "        encoder_embedding = self.encoder(images)\n",
        "        outputs = self.decoder(\n",
        "            inputs_embeds=encoder_embedding,\n",
        "            decoder_input_ids=captions_ids, \n",
        "            decoder_attention_mask=captions_len_mask,\n",
        "            return_dict=True\n",
        "        )\n",
        "        return outputs['logits']\n",
        "    \n",
        "    def training_step(self, x, batch_idx):\n",
        "        images, captions_ids, captions_len_mask  = x\n",
        "        logits = self(x)\n",
        "        #y_target = captions_ids[captions_len_mask == 1]\n",
        "        #loss = self.loss(logits, y_target)\n",
        "        loss = self.loss(logits.permute(0,2,1), captions_ids)\n",
        "\n",
        "        return {\"loss\": loss}\n",
        "    \n",
        "    def validation_step(self, x, batch_idx):\n",
        "        images, captions_ids, captions_len_mask  = x\n",
        "        logits = self(x)\n",
        "        #y_target = captions_ids[captions_len_mask == 1]\n",
        "        #loss = self.loss(logits, y_target)\n",
        "        loss = self.loss(logits.permute(0,2,1), captions_ids)\n",
        "\n",
        "        pred_seq = [TOKENIZER.decode(torch.argmax(sample, dim=-1))\n",
        "                    for sample in logits]\n",
        "        target_seq = [TOKENIZER.decode(target) for target in captions_ids]\n",
        "\n",
        "        return {\"val_loss\": loss,\n",
        "                \"pred_seq\": pred_seq,\n",
        "                \"target_seq\": target_seq}\n",
        "    \n",
        "    def test_step(self, x, batch_idx):\n",
        "        images, captions_ids, captions_len_mask  = x\n",
        "        logits = self(x)\n",
        "        #y_target = captions_ids[captions_len_mask == 1]\n",
        "        #loss = self.loss(logits, y_target)\n",
        "        loss = self.loss(logits.permute(0,2,1), captions_ids)\n",
        "\n",
        "        #pred_seq = TOKENIZER.decode(torch.argmax(logits, dim=-1))\n",
        "        #target_seq = TOKENIZER.decode(y_target)\n",
        "        pred_seq = [TOKENIZER.decode(torch.argmax(sample, dim=-1))\n",
        "                    for sample in logits]\n",
        "        # não usar tokenizer aqui, dá um jeito de pegar o original\n",
        "        # Eliminar o pad do score\n",
        "        target_seq = [TOKENIZER.decode(target) for target in captions_ids]\n",
        "\n",
        "        return {\"test_loss\": loss,\n",
        "                \"test_pred_seq\": pred_seq,\n",
        "                \"test_target_seq\": target_seq}\n",
        "    \n",
        "    def training_epoch_end(self, outputs):\n",
        "        avg_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
        "\n",
        "        self.log(\"avg_loss\", avg_loss, prog_bar=True)\n",
        "\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        # Metric 1\n",
        "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
        "\n",
        "        # Metric 2\n",
        "        predictions = outputs[0][\"pred_seq\"][::5]\n",
        "        targets = outputs[0][\"target_seq\"]\n",
        "        grouped_targets = [targets[start::5] for start in range(5)]\n",
        "\n",
        "        bleu = sacrebleu.corpus_bleu(predictions, grouped_targets)\n",
        "                \n",
        "        self.log(\"valid_loss\", avg_loss, prog_bar=True)\n",
        "        self.log(\"bleu\", bleu.score, prog_bar=True)\n",
        "        #self.log(\"bleu1234\", bleu.precisions, prog_bar=True)\n",
        "\n",
        "        print('Pred:', predictions[0])\n",
        "        print('Targets:', grouped_targets[0])\n",
        "\n",
        "        return {\"val_loss\": avg_loss, \"bleu\": bleu.score,\n",
        "                \"bleu1\": bleu.precisions[0], \"bleu2\": bleu.precisions[1],\n",
        "                \"bleu3\": bleu.precisions[2], \"bleu4\": bleu.precisions[3]\n",
        "                }\n",
        "\n",
        "    def test_epoch_end(self, outputs):\n",
        "        # Metric 1\n",
        "        avg_loss = torch.stack([x[\"test_loss\"] for x in outputs]).mean()\n",
        "\n",
        "        # Metric 2\n",
        "        predictions = [x[\"test_pred_seq\"] for x in outputs][::5]\n",
        "        targets = [x[\"test_target_seq\"] for x in outputs]\n",
        "        grouped_targets = [targets[start::5] for start in range(5)]\n",
        "\n",
        "        bleu = sacrebleu.corpus_bleu(predictions, grouped_targets)\n",
        "                    \n",
        "        self.log(\"test_loss\", avg_loss, prog_bar=True)\n",
        "        self.log(\"test_bleu\", bleu.score, prog_bar=True)\n",
        "        #self.log(\"bleu1234\", bleu.precisions, prog_bar=True)\n",
        "\n",
        "        print('Pred:', predictions[0])\n",
        "        print('Targets:', grouped_targets[0])\n",
        "\n",
        "        return {\"test_loss\": avg_loss, \"bleu\": bleu.score,\n",
        "                \"bleu1\": bleu.precisions[0], \"bleu2\": bleu.precisions[1],\n",
        "                \"bleu3\": bleu.precisions[2], \"bleu4\": bleu.precisions[3]\n",
        "                }\n",
        "\n",
        "            \n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5l0ScoFmF8Sj"
      },
      "source": [
        "def setup_logs_and_callbacks(output_path, exp_name):\n",
        "    tensorboard_path = os.path.join(output_path, \"logs\")\n",
        "    logs_folder = os.path.join(tensorboard_path, exp_name)\n",
        "    os.makedirs(logs_folder, exist_ok=True)\n",
        "    ckpt_path = os.path.join(logs_folder, \"-{epoch}\")\n",
        "\n",
        "    logger = TensorBoardLogger(tensorboard_path, exp_name)\n",
        "    checkpoint_callback = ModelCheckpoint(prefix=exp_name,\n",
        "                                          filepath=ckpt_path,\n",
        "                                          monitor='val_loss',\n",
        "                                          mode=\"min\")\n",
        "    return logger, checkpoint_callback"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBSJKe0tUHd8"
      },
      "source": [
        "# testando o treinamento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgUDao_aPFSs",
        "outputId": "8df77d57-a1f6-4bfc-91a8-20e40f7b242b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482,
          "referenced_widgets": [
            "0788abcabe5540b59e7f41e827b3dade",
            "632d71b156d948b792125e585375cbfc",
            "bf4561800946463e9ff76026e587a16d",
            "8f4ea07ec84c45dc90a6e62af1805704",
            "cd1b9f19b4414240b12c9e76e6589285",
            "e7ccee7cf0694b47a6adf602b636cb8c",
            "c61519c9a01c4aaaa94d3e09153b1179",
            "28832e62e03d433f96158260c8185308",
            "c42548e61f4c4bd6ad153fd1e43dc3bb",
            "16308db968e14b75a02f4ad5e2ace84f",
            "ac9e30ab5c3a4410ad3d0dbb46238996",
            "f893473e83a84fbaaff3183a64b57531",
            "254a8279a46846fe91e6fb526c1eec83",
            "ffa4ebaa6f8643c39dd5f4fe1257ab0b",
            "f6f2905020f94437bdf4e373adc67a78",
            "c5134cbc84a1449a9275aa0c741f114b"
          ]
        }
      },
      "source": [
        "model = ImageCaptioning()\n",
        "trainer = pl.Trainer(fast_dev_run=True, gpus=1)\n",
        "trainer.fit(model, datamodule=DATALOADER_MD)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded pretrained weights for efficientnet-b0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Running in fast_dev_run mode: will run a full train, val and test loop using a single batch\n",
            "\n",
            "  | Name    | Type                | Params\n",
            "------------------------------------------------\n",
            "0 | encoder | ImageEncoder        | 14 M  \n",
            "1 | decoder | EncoderDecoderModel | 247 M \n",
            "2 | loss    | CrossEntropyLoss    | 0     \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0788abcabe5540b59e7f41e827b3dade",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c42548e61f4c4bd6ad153fd1e43dc3bb",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Pred: midfield midfield midfield midfield midfield midfield midfield midfield midfield midfield indicative midfield midfield midfield midfield midfield midfield midfield midfield midfield midfield midfield midfield midfield midfield midfield midfield midfield midfield midfield midfield midfield midfield midfield midfield midfield midfield midfield midfield midfield midfield midfield midfield midfield midfield midfield midfield midfield midfield\n",
            "Targets: ['[CLS] a woman in a room with a cat. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', '[CLS] a group of people riding on the back of a loaded red pickup truck. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: The validation_epoch_end should not return anything as of 9.1.to log, use self.log(...) or self.write(...) directly in the LightningModule\n",
            "  warnings.warn(*args, **kwargs)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7g3Wz1_Bzkq"
      },
      "source": [
        "# treinamento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vrs-zim6OzT9"
      },
      "source": [
        "logger, checkpoint_callback = setup_logs_and_callbacks('/content/testing', 'testing')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzeu1HK9uMyj"
      },
      "source": [
        "#trainer = Trainer(\n",
        "#    resume_from_checkpoint=None,\n",
        "#    gpus=1,\n",
        "#    checkpoint_callback=checkpoint_callback,\n",
        "#    early_stop_callback=False,\n",
        "#    logger=logger,\n",
        "#    accumulate_grad_batches=ACC_GRAD_BATCHES,\n",
        "#    max_epochs=MAX_EPOCHS,\n",
        "    #val_check_interval=VALIDATION_CHECK,\n",
        "#    progress_bar_refresh_rate=PROGESS_BAR_RATE,\n",
        "#)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}